{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'w2' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_loss(output, y)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Backpropagation and weights update\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m backpropagate(X, a1, y, output, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Print the loss every 1000 epochs\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m, in \u001b[0;36mbackpropagate\u001b[1;34m(x, a1, y_true, y_pred, learning_rate)\u001b[0m\n\u001b[0;32m     55\u001b[0m d_output \u001b[38;5;241m=\u001b[39m error_output \u001b[38;5;241m*\u001b[39m sigmoid_derivative(y_pred)  \u001b[38;5;66;03m# Gradient of output layer\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Hidden layer error\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m error_hidden \u001b[38;5;241m=\u001b[39m d_output\u001b[38;5;241m.\u001b[39mdot(w2\u001b[38;5;241m.\u001b[39mT)  \u001b[38;5;66;03m# Error propagated back to hidden layer\u001b[39;00m\n\u001b[0;32m     59\u001b[0m d_hidden \u001b[38;5;241m=\u001b[39m error_hidden \u001b[38;5;241m*\u001b[39m sigmoid_derivative(a1)  \u001b[38;5;66;03m# Gradient of hidden layer\u001b[39;00m\n\u001b[0;32m     63\u001b[0m w2 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m a1\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(d_output) \u001b[38;5;241m*\u001b[39m learning_rate  \u001b[38;5;66;03m# Update weight for hidden to output\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'w2' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "# Let's create some synthetic data for house sizes and corresponding prices\n",
    "np.random.seed(42)\n",
    "\n",
    "# House sizes (in square feet)\n",
    "X = np.array([[500], [1000], [1500], [2000], [2500], [3000]])\n",
    "\n",
    "# Corresponding house prices (in dollars, with some added noise)\n",
    "# For example: Price = size * 150 + noise\n",
    "y = X * 150 + (np.random.randn(*X.shape) * 10000)  # Random noise\n",
    "\n",
    "\n",
    "\n",
    "# Neural Network Parameters\n",
    "input_neurons = 1\n",
    "hidden_neurons = 1\n",
    "output_neurons = 1\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "w1 = np.random.randn(input_neurons, hidden_neurons)  # Weight for input to hidden\n",
    "b1 = np.random.randn(hidden_neurons)  # Bias for hidden layer\n",
    "\n",
    "w2 = np.random.randn(hidden_neurons, output_neurons)  # Weight for hidden to output\n",
    "b2 = np.random.randn(output_neurons)  # Bias for output layer\n",
    "\n",
    "\n",
    "\n",
    "def forward_pass(x):\n",
    "    # Input to hidden layer\n",
    "    z1 = np.dot(x, w1) + b1\n",
    "    a1 = sigmoid(z1)  # Apply sigmoid activation function\n",
    "    \n",
    "    # Hidden to output layer\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    output = sigmoid(z2)  # Apply sigmoid activation function\n",
    "    \n",
    "    return output, a1\n",
    "\n",
    "\n",
    "def compute_loss(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "\n",
    "def backpropagate(x, a1, y_true, y_pred, learning_rate=0.01):\n",
    "    # Output layer error\n",
    "    error_output = y_pred - y_true\n",
    "    d_output = error_output * sigmoid_derivative(y_pred)  # Gradient of output layer\n",
    "    \n",
    "    # Hidden layer error\n",
    "    error_hidden = d_output.dot(w2.T)  # Error propagated back to hidden layer\n",
    "    d_hidden = error_hidden * sigmoid_derivative(a1)  # Gradient of hidden layer\n",
    "    \n",
    "\n",
    "    \n",
    "    w2 -= a1.T.dot(d_output) * learning_rate  # Update weight for hidden to output\n",
    "    b2 -= np.sum(d_output, axis=0) * learning_rate  # Update bias for output layer\n",
    "    \n",
    "    w1 -= x.T.dot(d_hidden) * learning_rate  # Update weight for input to hidden\n",
    "    b1 -= np.sum(d_hidden, axis=0) * learning_rate  # Update bias for hidden layer\n",
    "\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    output, a1 = forward_pass(X)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(output, y)\n",
    "    \n",
    "    # Backpropagation and weights update\n",
    "    backpropagate(X, a1, y, output, learning_rate=0.01)\n",
    "    \n",
    "    # Print the loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "# Predictions after training\n",
    "output, _ = forward_pass(X)\n",
    "\n",
    "# Plotting the data\n",
    "plt.scatter(X, y, color='blue', label='Actual prices')  # Actual prices (blue dots)\n",
    "plt.plot(X, output, color='red', label='Predicted prices')  # Predicted prices (red line)\n",
    "plt.xlabel('House Size (sq ft)')\n",
    "plt.ylabel('House Price ($)')\n",
    "plt.title('House Price Prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on new data\n",
    "test_sizes = np.array([[1200], [1600], [2200], [2700]])\n",
    "predicted_prices, _ = forward_pass(test_sizes)\n",
    "\n",
    "# Display the predictions\n",
    "print(\"Predicted prices for test sizes:\")\n",
    "for size, price in zip(test_sizes, predicted_prices):\n",
    "    print(f\"Size: {size[0]} sq ft -> Predicted Price: ${price[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
